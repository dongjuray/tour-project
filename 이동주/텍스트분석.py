# -*- coding: utf-8 -*-
"""텍스트분석.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u7EoviRyuitzrHukKrR5Bo1Zi2_vhDEY
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install koreanize-matplotlib
# import koreanize_matplotlib

!pip install gensim

import pandas as pd

# 파일 경로와 장소 이름 (업로드된 전통문화 3개 파일 기준)
files_info = [
    ("/content/bulguksa.csv", "bulguksa"),
    ("/content/andong_village.csv", "andong")
]

# 각 파일 처리
dfs = []
for path, place in files_info:
    try:
        df = pd.read_csv(path, encoding="utf-8", header=None)
    except UnicodeDecodeError:
        df = pd.read_csv(path, encoding="ISO-8859-1", header=None)

    # 첫 줄이 헤더거나 BOM 있으면 제거
    if df.iloc[0, 0].lower().strip() in ['review', 'ï»¿review']:
        df = df.iloc[1:]

    # 여러 열이면 한 줄로 결합
    reviews = []
    for row in df.itertuples(index=False):
        line = " ".join(map(str, row)).strip()
        reviews.append(line)

    all_df = pd.DataFrame({"review": reviews})
    all_df["place"] = place
    dfs.append(all_df)

# 병합
df_all = pd.concat(dfs, ignore_index=True)

# 확인
print(df_all.head())

import pandas as pd

# 파일 경로와 장소 이름 (업로드된 전통문화 3개 파일 기준)
files_info = [
    ("/content/gangwon_mt.csv", "Gangwon"),
    ("/content/namiIsland.csv", "Nami")
]

# 각 파일 처리
dfs = []
for path, place in files_info:
    try:
        df = pd.read_csv(path, encoding="utf-8", header=None)
    except UnicodeDecodeError:
        df = pd.read_csv(path, encoding="ISO-8859-1", header=None)

    # 첫 줄이 헤더거나 BOM 있으면 제거
    if df.iloc[0, 0].lower().strip() in ['review', 'ï»¿review']:
        df = df.iloc[1:]

    # 여러 열이면 한 줄로 결합
    reviews = []
    for row in df.itertuples(index=False):
        line = " ".join(map(str, row)).strip()
        reviews.append(line)

    temp_df = pd.DataFrame({"review": reviews})
    temp_df["place"] = place
    dfs.append(temp_df)

# 병합
df_traditional = pd.concat(dfs, ignore_index=True)

# 확인
print(df_traditional.head())

import re
import nltk
from nltk.corpus import stopwords

# nltk 리소스 (처음 1번만 실행)
nltk.download('stopwords')
nltk.download('punkt')

# 불용어 설정
stop_words = set(stopwords.words('english'))

def preprocess_text_simple(text):
    text = text.lower()
    text = re.sub(r"[^a-z\s]", "", text)
    words = text.split()  # 간단한 공백 기준 토큰화
    words = [w for w in words if w not in stop_words and len(w) > 1]
    return " ".join(words)

df_all["cleaned_review"] = df_all["review"].apply(preprocess_text_simple)

from textblob import TextBlob

# 감성 점수 부여 함수
def get_sentiment(text):
    return TextBlob(text).sentiment.polarity  # -1 ~ +1

# 적용
df_all["sentiment_score"] = df_all["cleaned_review"].apply(get_sentiment)

# 감성 라벨 (optional: 긍/중/부)
def label_sentiment(score):
    if score > 0.1:
        return "positive"
    elif score < -0.1:
        return "negative"
    else:
        return "neutral"

df_all["sentiment_label"] = df_all["sentiment_score"].apply(label_sentiment)

df_all[df_all['sentiment_label']=='positive'].head()

import matplotlib.pyplot as plt

# 1. 감성 라벨 분포 (막대그래프)
label_counts = df_all["sentiment_label"].value_counts()

plt.figure(figsize=(6, 4))
label_counts.plot(kind='bar', color=['green', 'gray', 'red'])
plt.title("Sentiment Label Distribution")
plt.xlabel("Sentiment")
plt.ylabel("Number of Reviews")
plt.xticks(rotation=0)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

# 2. 감성 점수 분포 (히스토그램)
plt.figure(figsize=(6, 4))
plt.hist(df_all["sentiment_score"], bins=20, color='skyblue', edgecolor='black')
plt.title("Sentiment Score Distribution")
plt.xlabel("Polarity Score (-1 to +1)")
plt.ylabel("Number of Reviews")
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

from textblob import TextBlob

# 감성 점수 계산 함수
def get_sentiment(text):
    return TextBlob(text).sentiment.polarity  # -1 ~ +1

# 점수 적용
df_all["sentiment_score"] = df_all["cleaned_review"].apply(get_sentiment)

# 세부 감성 라벨 함수
def detailed_sentiment(score):
    if score >= 0.4:
        return "high-positive"
    elif score >= 0.1:
        return "low-positive"
    elif score > -0.1:
        return "neutral"
    else:
        return "negative"

# 감성 구간별 라벨 부여
df_all["sentiment_detail"] = df_all["sentiment_score"].apply(detailed_sentiment)

from wordcloud import WordCloud
import matplotlib.pyplot as plt


# 1. 긍정(high-positive) 리뷰 텍스트 결합
positive_text = " ".join(df_all[df_all["sentiment_detail"] == "high-positive"]["cleaned_review"])

# 2. 부정(negative) 리뷰 텍스트 결합
negative_text = " ".join(df_all[df_all["sentiment_detail"] == "negative"]["cleaned_review"])

# 3. 긍정 워드클라우드 생성
wordcloud_pos = WordCloud(
    width=800,
    height=400,
    background_color='white',
    colormap='Reds'  # 붉은 계열
).generate(positive_text)

# 4. 부정 워드클라우드 생성
wordcloud_neg = WordCloud(
    width=800,
    height=400,
    background_color='white',
    colormap='Greys'  # 회색 계열
).generate(negative_text)

# 5. 시각화: 긍정
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud_pos, interpolation='bilinear')
plt.axis("off")
plt.title("긍정 리뷰 워드클라우드", fontsize=16)
plt.show()

# 6. 시각화: 부정
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud_neg, interpolation='bilinear')
plt.axis("off")
plt.title("부정 리뷰 워드클라우드", fontsize=16)
plt.show()

# 리뷰 수 기준으로 나눠서 source 할당 (예: bulguksa: 0~N, hahoe: N~)
import seaborn as sns

N = 250  # 불국사 리뷰 수 (정확한 수로 교체해야 함)

df_all.loc[:N-1, "source"] = "Bulguksa"
df_all.loc[N:, "source"] = "Hahoe"

plt.figure(figsize=(8, 4))
sns.countplot(data=df_all, x="sentiment_label", hue="source", palette="pastel")
plt.title("불국사 vs 하회마을 감성 라벨 분포")
plt.xlabel("Sentiment")
plt.ylabel("Review Count")
plt.legend(title="Place")
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import networkx as nx
from collections import Counter
from itertools import combinations
from gensim.utils import simple_preprocess

# 토큰화
df_all["tokens"] = df_all["cleaned_review"].apply(lambda x: simple_preprocess(x))


# 1. 긍정(high-positive) 리뷰만 필터링
tokens_pos = df_all[df_all["sentiment_detail"] == "high-positive"]["tokens"]

# 2. 공통 등장 단어쌍 추출
cooccurrence_pos = []
for tokens in tokens_pos:
    tokens = list(set(tokens))  # 중복 제거
    cooccurrence_pos += list(combinations(tokens, 2))

# 3. 단어쌍 빈도 계산
pair_counts_pos = Counter(cooccurrence_pos)
top_pairs_pos = pair_counts_pos.most_common(100)  # 상위 100개 단어쌍

# 4. 네트워크 생성
G_pos = nx.Graph()
for (w1, w2), count in top_pairs_pos:
    G_pos.add_edge(w1, w2, weight=count)

# 5. 시각화
plt.figure(figsize=(12, 10))
pos = nx.spring_layout(G_pos, k=0.5, seed=42)  # 배치 고정
weights = [d['weight'] for (u, v, d) in G_pos.edges(data=True)]

nx.draw(
    G_pos, pos,
    with_labels=True,
    node_size=700,
    font_size=10,
    edge_color=weights,
    edge_cmap=plt.cm.Reds,
    width=2,
    edge_vmin=min(weights),
    edge_vmax=max(weights)

)

plt.title("positive 리뷰 기반 의미연결망")
plt.show()

#negative 리뷰만 필터링
tokens_high = df_all[df_all["sentiment_detail"] == "negative"]["tokens"]

#공통 등장 단어쌍 추출
cooccurrence_high = []
for tokens in tokens_high:
    tokens = list(set(tokens))  # 중복 제거
    cooccurrence_high += list(combinations(tokens, 2))

#단어쌍 빈도 계산
pair_counts_high = Counter(cooccurrence_high)
top_pairs_high = pair_counts_high.most_common(100)

#네트워크 생성
G_high = nx.Graph()
for (w1, w2), count in top_pairs_high:
    G_high.add_edge(w1, w2, weight=count)

#시각화
plt.figure(figsize=(12, 10))
pos = nx.spring_layout(G_high, k=0.5, seed=42)
weights = [d['weight'] for (u, v, d) in G_high.edges(data=True)]

nx.draw(G_high, pos, with_labels=True, node_size=700, font_size=10,
        edge_color=weights, edge_cmap=plt.cm.Greens, width=2,
        edge_vmin=min(weights), edge_vmax=max(weights))
plt.title("negative 리뷰 기반 의미연결망")
plt.show()

# 1. 부정 리뷰만 필터링
df_negative = df_all[df_all["sentiment_detail"] == "negative"]

# 2. 주요 컬럼만 선택해 보기 좋게 정리
df_negative_subset = df_negative[['review', 'cleaned_review', 'sentiment_score']]

# 3. 상위 5개 미리 보기
print(df_negative_subset.head())

# (선택) 4. CSV로 저장하고 싶을 경우
# df_negative_subset.to_csv("negative_reviews_only.csv", index=False)

df_negative[df_negative['source'] == 'Bulguksa']  # 불국사 부정 리뷰만

df_negative[df_negative['source'] == 'Hahoe']     # 하회마을 부정 리뷰만

from collections import Counter

# 긍정 리뷰 핵심 단어
positive_tokens = df_all[df_all["sentiment_detail"] == "high-positive"]["tokens"]
positive_words = [word for tokens in positive_tokens for word in tokens]
positive_counter = Counter(positive_words)
top_positive_words = positive_counter.most_common(20)

# 부정 리뷰 핵심 단어
negative_tokens = df_all[df_all["sentiment_detail"] == "negative"]["tokens"]
negative_words = [word for tokens in negative_tokens for word in tokens]
negative_counter = Counter(negative_words)
top_negative_words = negative_counter.most_common(20)

# 결과 출력
print("✅ 긍정 리뷰 핵심 단어 Top 20")
for word, count in top_positive_words:
    print(f"{word}: {count}")

print("\n❌ 부정 리뷰 핵심 단어 Top 20")
for word, count in top_negative_words:
    print(f"{word}: {count}")

import networkx as nx
from collections import Counter
from itertools import combinations

# 1. 긍정(high-positive) 의미연결망 구성
tokens_pos = df_all[df_all["sentiment_detail"] == "high-positive"]["tokens"]
cooccurrence_pos = []
for tokens in tokens_pos:
    cooccurrence_pos += list(combinations(set(tokens), 2))  # 중복 제거된 단어쌍

pair_counts_pos = Counter(cooccurrence_pos)
top_pairs_pos = pair_counts_pos.most_common(100)

G_pos_top = nx.Graph()
for (w1, w2), count in top_pairs_pos:
    G_pos_top.add_edge(w1, w2, weight=count)

# 2. 부정(negative) 의미연결망 구성
tokens_neg = df_all[df_all["sentiment_detail"] == "negative"]["tokens"]
cooccurrence_neg = []
for tokens in tokens_neg:
    cooccurrence_neg += list(combinations(set(tokens), 2))

pair_counts_neg = Counter(cooccurrence_neg)
top_pairs_neg = pair_counts_neg.most_common(100)

G_neg_top = nx.Graph()
for (w1, w2), count in top_pairs_neg:
    G_neg_top.add_edge(w1, w2, weight=count)

# 3. 중심성 계산 (Degree Centrality 기준)
pos_degree_centrality = nx.degree_centrality(G_pos_top)
top_pos_nodes = sorted(pos_degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10]

neg_degree_centrality = nx.degree_centrality(G_neg_top)
top_neg_nodes = sorted(neg_degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10]

# 4. 결과 출력
print("✅ 긍정 리뷰 핵심 단어 Top 10 (degree 중심성 기준)")
for word, score in top_pos_nodes:
    print(f"{word}: {score:.3f}")

print("\n❌ 부정 리뷰 핵심 단어 Top 10 (degree 중심성 기준)")
for word, score in top_neg_nodes:
    print(f"{word}: {score:.3f}")

traditional_paths = [
    "/content/namiIsland.csv",
    "/content/gangwon_mt.csv"
]

traditional_dfs = []
for path in traditional_paths:
    df = pd.read_csv(path, encoding="ISO-8859-1", header=None)

    # BOM 문제로 첫 줄이 컬럼명인 경우 제거
    if df.iloc[0, 0].lower().strip() in ['review', 'ï»¿review']:
        df = df.iloc[1:]  # 첫 줄 제거

    df.columns = ["review"]
    traditional_dfs.append(df)

# 병합
df_traditional = pd.concat(traditional_dfs, ignore_index=True)

# 상위 5개 확인
print(df_traditional.head())

import re
from textblob import TextBlob

# 간단한 불용어 리스트 정의 (직접 작성한 최소 버전)
basic_stopwords = {
    "the", "and", "is", "in", "it", "of", "to", "a", "i", "this", "was", "for", "on",
    "with", "we", "they", "at", "an", "as", "are", "you", "be", "not", "that", "but"
}

# 전처리 함수 (NLTK 없이)
def preprocess_text_simple(text):
    if pd.isna(text):
        return ""
    text = text.lower()
    text = re.sub(r"[^a-z\s]", "", text)
    words = text.split()
    words = [w for w in words if w not in basic_stopwords and len(w) > 1]
    return " ".join(words)

# 전처리 및 토큰화
df_traditional["cleaned_review"] = df_traditional["review"].apply(preprocess_text_simple)
df_traditional["tokens"] = df_traditional["cleaned_review"].apply(lambda x: x.split())

# 감성 점수 계산
df_traditional["sentiment_score"] = df_traditional["cleaned_review"].apply(lambda x: TextBlob(x).sentiment.polarity)

# 감성 라벨링
def detailed_sentiment(score):
    if score >= 0.4:
        return "high-positive"
    elif score >= 0.1:
        return "low-positive"
    elif score > -0.1:
        return "neutral"
    else:
        return "negative"

df_traditional["sentiment_detail"] = df_traditional["sentiment_score"].apply(detailed_sentiment)

# 결과 미리보기
df_traditional[["review", "cleaned_review", "sentiment_score", "sentiment_detail"]].head()

import pandas as pd
import re



# 한글 포함 여부를 판별하는 함수
def contains_korean(text):
    return bool(re.search(r"[가-힣]", str(text)))

# 한국어가 포함된 행 제거
df_cleaned = df_traditional[~df_traditional['review'].apply(contains_korean)].reset_index(drop=True)

# 결과 저장 (필요 시 파일로 저장 가능)
df_cleaned.to_csv("/content/namiIsland_no_korean.csv", index=False, encoding="utf-8-sig")
print("✅ 한국어가 포함된 리뷰 제거 완료: /content/namiIsland_no_korean.csv")

import pandas as pd

# 유적지와 전통문화 리뷰 합치기
combined_df = pd.concat([df_all, df_traditional], ignore_index=True)

# 결과 확인
print(combined_df["source"].value_counts())  # 출처별 개수 확인
combined_df.head()

import pandas as pd
import matplotlib.pyplot as plt
import networkx as nx
from gensim.utils import simple_preprocess
from collections import Counter
from itertools import combinations

# 1. 간단한 불용어 리스트 정의
custom_stopwords = set([
    "the", "and", "is", "was", "it", "of", "to", "a", "in", "that", "for", "on",
    "with", "this", "as", "at", "an", "be", "but", "we", "they", "i", "our", "my",
    "you", "had", "were", "are", "have", "from", "so", "very", "if", "or", "not",
    "there", "when", "what", "which", "has", "also", "just", "out", "more", "their"
])

# 2. 텍스트 전처리 함수
def tokenize(text):
    if pd.isna(text):
        return []
    tokens = simple_preprocess(text)
    return [word for word in tokens if word not in custom_stopwords and len(word) > 1]

# 3. 긍정 리뷰만 추출
pos_df = combined_df[combined_df["sentiment_label"] == "positive"].copy()
pos_df["tokens"] = pos_df["cleaned_review"].apply(tokenize)

# 4. 전통문화 vs 유적지 분리
tokens_heritage = pos_df[pos_df["source"].isin(["Bulguksa", "Hahoe"])]["tokens"]
tokens_traditional = pos_df[pos_df["source"].str.startswith("K-Traditional")]["tokens"]

# 5. 의미연결망 생성 함수
def build_cooccurrence_graph(token_lists, top_n=50):
    pairs = []
    for tokens in token_lists:
        tokens = list(set(tokens))
        pairs.extend(combinations(tokens, 2))
    pair_counts = Counter(pairs).most_common(top_n)

    G = nx.Graph()
    for (w1, w2), count in pair_counts:
        G.add_edge(w1, w2, weight=count)
    return G

# 6. 그래프 생성
G_pos_heritage = build_cooccurrence_graph(tokens_heritage)
G_pos_traditional = build_cooccurrence_graph(tokens_traditional)

# 7. 시각화 함수
def draw_network(G, title, cmap):
    if len(G.edges) == 0:
        print(f"[{title}] 의미연결망에 유효한 단어쌍이 없어 시각화할 수 없습니다.")
        return
    plt.figure(figsize=(10, 8))
    pos = nx.spring_layout(G, k=0.5, seed=42)
    weights = [d['weight'] for (u, v, d) in G.edges(data=True)]
    nx.draw(G, pos, with_labels=True, node_size=700, font_size=10,
            edge_color=weights, edge_cmap=cmap, width=2,
            edge_vmin=min(weights), edge_vmax=max(weights))
    plt.title(title)
    plt.show()

# 8. 시각화
draw_network(G_pos_heritage, "유적지 긍정 리뷰 의미연결망", plt.cm.Blues)
draw_network(G_pos_traditional, "전통문화 긍정 리뷰 의미연결망", plt.cm.Reds)

import networkx as nx
import matplotlib.pyplot as plt
from collections import Counter
from itertools import combinations

# 1. 부정 리뷰만 필터링
neg_df = combined_df[combined_df["sentiment_label"] == "negative"].copy()

# 2. 토큰화 함수 (이전에 정의되어 있어야 함)
def tokenize(text):
    return [word for word in text.lower().split() if word.isalpha() and len(word) > 1]

neg_df["tokens"] = neg_df["cleaned_review"].apply(tokenize)

# 3. 전통문화 vs 유적지로 나누기
tokens_heritage_neg = neg_df[neg_df["source"].isin(["Bulguksa", "Hahoe"])]["tokens"]
tokens_traditional_neg = neg_df[neg_df["source"].str.startswith("K-Traditional")]["tokens"]

# 4. 의미연결망 생성 함수
def build_cooccurrence_graph(token_lists, top_n=50):
    pair_counter = Counter()
    for tokens in token_lists:
        unique_tokens = list(set(tokens))  # 중복 제거
        pairs = combinations(unique_tokens, 2)
        pair_counter.update(pairs)
    top_pairs = pair_counter.most_common(top_n)

    G = nx.Graph()
    for (w1, w2), count in top_pairs:
        G.add_edge(w1, w2, weight=count)
    return G

# 5. 시각화 함수
def draw_network(G, title, cmap):
    if len(G.edges) == 0:
        print(f"[{title}] 연결된 단어쌍이 없어 시각화할 수 없습니다.")
        return
    pos = nx.spring_layout(G, k=0.5, seed=42)
    weights = [d['weight'] for (u, v, d) in G.edges(data=True)]

    plt.figure(figsize=(12, 10))
    nx.draw(
        G, pos, with_labels=True, node_size=700, font_size=10,
        edge_color=weights, edge_cmap=cmap, width=2,
        edge_vmin=min(weights), edge_vmax=max(weights)
    )
    plt.title(title)
    plt.show()

# 6. 의미연결망 생성 및 시각화
G_neg_heritage = build_cooccurrence_graph(tokens_heritage_neg, top_n=50)
G_neg_traditional = build_cooccurrence_graph(tokens_traditional_neg, top_n=50)

draw_network(G_neg_heritage, "유적지 부정 리뷰 의미연결망", plt.cm.Reds)
draw_network(G_neg_traditional, "전통문화 부정 리뷰 의미연결망", plt.cm.Purples)

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# 1. 긍정 리뷰 필터링 및 연결
pos_reviews = combined_df[combined_df["sentiment_label"] == "positive"]
text_pos = " ".join(pos_reviews["cleaned_review"].dropna())

# 2. 부정 리뷰 필터링 및 연결
neg_reviews = combined_df[combined_df["sentiment_label"] == "negative"]
text_neg = " ".join(neg_reviews["cleaned_review"].dropna())

# 3. 워드클라우드 생성
wc_pos = WordCloud(
    width=800, height=400, background_color='white', colormap='Greens'
).generate(text_pos)

wc_neg = WordCloud(
    width=800, height=400, background_color='white', colormap='Reds'
).generate(text_neg)

# 4. 시각화
plt.figure(figsize=(16, 8))

# 긍정 워드클라우드
plt.subplot(1, 2, 1)
plt.imshow(wc_pos, interpolation='bilinear')
plt.axis("off")
plt.title("긍정 리뷰 워드클라우드")

# 부정 워드클라우드
plt.subplot(1, 2, 2)
plt.imshow(wc_neg, interpolation='bilinear')
plt.axis("off")
plt.title("부정 리뷰 워드클라우드")

plt.tight_layout()
plt.show()

"""1. 긍정 의미연결망 & 워드클라우드 해석

✅ **전반적 경향**
- 긍정 감성의 리뷰에서는 beautiful, peaceful, experience, traditional, must, view, nature, calm, history 등의 단어들이 서로 강하게 연결되어 있었음.

- 워드클라우드에서도 beautiful, place, visit, recommend, amazing, enjoy 등의 단어가 크게 나타났고, 이는 장소의 아름다움과 추천 의향이 많이 언급되었음을 뜻해.

- 🏯 유적지 리뷰(불국사/하회마을) 중심 단어
temple, architecture, history, heritage, walk, calm
→ 고요한 분위기, 전통적 건축물, 역사성이 강점으로 인식됨.

- 🎎 전통문화 리뷰(한복·체험 중심) 중심 단어
hanbok, experience, wear, photo, try, traditional, fun, beautiful
→ 체험 자체의 즐거움과 사진 찍는 활동이 긍정적으로 평가됨.

🔍 2. 부정 의미연결망 & 워드클라우드 해석

❗ **전반적 경향**
- 부정 감성에서는 crowded, hot, long, wait, tired, small, expect, not worth, boring 등 불편함과 기대 미달 관련 단어들이 연결되어 있었음.

- 워드클라우드 역시 crowded, disappointed, nothing, hard, waste 등의 단어가 부각됨.

- 🏯 유적지 리뷰 부정 키워드
crowded, tourist, hot, hard, walk, boring
→ 여름철 방문의 피로감, 혼잡함, 관광지 상업화에 대한 피로가 드러남.

- 🎎 전통문화 리뷰 부정 키워드
리뷰 수는 적었지만 expected, not much, too short, touristy 등
→ 체험의 깊이 부족이나 관광 상품화된 체험에 대한 아쉬움이 나타남.
"""

